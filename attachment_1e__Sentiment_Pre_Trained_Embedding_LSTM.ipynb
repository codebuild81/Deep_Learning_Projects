{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1e. Sentiment_Pre_Trained_Embedding_LSTM.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"QK6Wtu8zR7W0","colab_type":"text"},"cell_type":"markdown","source":["Set the seed"]},{"metadata":{"id":"HKo_FGWlR7W1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7gZ3PLlVR7XA","colab_type":"text"},"cell_type":"markdown","source":["Data can be downloaded from Kaggle -> https://www.kaggle.com/c/word2vec-nlp-tutorial/data"]},{"metadata":{"id":"kxenUGDhR7W9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('kaggle/labeledTrainData.tsv.zip',  #filepath\n","                 header=0, delimiter=\"\\t\", quoting=3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9Es0MIr8R7XF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["df.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LuOuT3-0R7XR","colab_type":"text"},"cell_type":"markdown","source":["## Data Preprocessing"]},{"metadata":{"id":"JaF0c1HMR7XI","colab_type":"text"},"cell_type":"markdown","source":["1.Split Data into Training and Test Data"]},{"metadata":{"id":"VoYsGAgRR7XJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['review'],\n","    df['sentiment'],\n","    test_size=0.2, \n","    random_state=42\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_VYq8asrSyiN","colab_type":"text"},"cell_type":"markdown","source":["2.Build Tokenizer to get Number sequences for Each review"]},{"metadata":{"id":"-Y0jqE-hR7XS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","\n","#Vocab size\n","top_words = 10000\n","\n","t = Tokenizer(num_words=top_words)\n","t.fit_on_texts(X_train.tolist())\n","\n","#Get the word index for each of the word in the review\n","X_train = t.texts_to_sequences(X_train.tolist())\n","X_test = t.texts_to_sequences(X_test.tolist())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tbpwqn6iR7Xg","colab_type":"text"},"cell_type":"markdown","source":["3.Pad sequences to make each review size equalGet the word index for each of the word in the review"]},{"metadata":{"id":"D79z-4uoR7Xn","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.preprocessing import sequence\n","\n","#Each review size\n","max_review_length = 300\n","\n","X_train = sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9NWBvhFUR7Xw","colab_type":"text"},"cell_type":"markdown","source":["## Build Embedding Matrix from Pre-Trained Word2Vec model"]},{"metadata":{"id":"6pcLpvW5R7Xw","colab_type":"text"},"cell_type":"markdown","source":["Load pre-trained Gensim Embeddings"]},{"metadata":{"id":"rYebjVaXR7Xy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Install gensim\n","!pip install gensim --quiet\n","\n","#Load pre-trained model\n","import gensim\n","word2vec = gensim.models.Word2Vec.load('word2vec-movie-50')\n","\n","#Embedding Length\n","embedding_vector_length = word2vec.wv.vectors.shape[1]\n","\n","print('Loaded word2vec model..')\n","print('Model shape: ', word2vec.wv.vectors.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2V5JPOl5R7X_","colab_type":"text"},"cell_type":"markdown","source":["Build matrix for current data"]},{"metadata":{"id":"OLL-LUZJR7YA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Initialize embedding matrix to all zeros\n","embedding_matrix = np.zeros((top_words + 1, #Vocablury size + 1\n","                             embedding_vector_length))\n","\n","#Steps for populating embedding matrix\n","\n","#1. Check each word in tokenizer vocablury to see if it exist in pre-trained\n","# word2vec model.\n","#2. If found, update embedding matrix with embeddings for the word \n","# from word2vec model\n","\n","for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n","    if i > top_words:\n","        break\n","    if word in word2vec.wv.vocab:\n","        embedding_vector = word2vec.wv[word]\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xdbd5_qXR7YE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Check embeddings for word 'great'\n","embedding_matrix[t.word_index['great']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v6vR1CQfR7YQ","colab_type":"text"},"cell_type":"markdown","source":["## Build the Graph"]},{"metadata":{"id":"YDaaL9r5R7YR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dropout, Dense, Embedding, Flatten, LSTM\n","\n","#Build a sequential model\n","model = Sequential()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"086FRu-gR7Yc","colab_type":"text"},"cell_type":"markdown","source":["Add Embedding layer"]},{"metadata":{"id":"SVOiv1D7R7Yd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model.add(Embedding(top_words + 1,\n","                    embedding_vector_length,\n","                    input_length=max_review_length,\n","                    weights=[embedding_matrix], #Pre-trained embedding\n","                    trainable=False) #We do not want to change embedding\n","         )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vZzSadvdR7Yf","colab_type":"text"},"cell_type":"markdown","source":["Output from Embedding is 3 dimension \n","- batch_size x max_review_length x embedding_vector_length. "]},{"metadata":{"id":"HR-kut27R7Yg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Add Layer with 100 LSTM Memory Units\n","model.add(LSTM(100, \n","               dropout=0.2, #Dropout applied prior to feeding input to LSTM\n","               recurrent_dropout=0.2)) #Droput applied to the output of LSTM\n","\n","#Output Layer\n","model.add(Dense(1,activation='sigmoid'))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SkielaGfR7Y9","colab_type":"text"},"cell_type":"markdown","source":["## Execute the graph"]},{"metadata":{"id":"GtXiww1sR7Y9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model.fit(X_train,y_train,\n","          epochs=1,\n","          batch_size=128,          \n","          validation_data=(X_test, y_test),\n","          verbose=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nbBGh2DlR7ZA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model.predict(X_test[100:102])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K5D8mTbhIfrX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]}]}