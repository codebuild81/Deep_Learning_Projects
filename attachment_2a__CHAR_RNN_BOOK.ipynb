{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2a. CHAR_RNN_BOOK.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FjMJKaBXi9ru","colab_type":"text"},"cell_type":"markdown","source":["### Load the data"]},{"metadata":{"id":"NdRMc0S0qZlv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X9iOqzZijTpr","colab_type":"text"},"cell_type":"markdown","source":["Download data from Project Gutenberg site -> http://www.gutenberg.org/files/1342/1342-0.txt"]},{"metadata":{"id":"USE1lx0aqZl3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Download book\n","!wget -O Pride_and_Prejudice.txt http://www.gutenberg.org/files/1342/1342-0.txt --quiet\n","\n","#Read it as string\n","book_text = open('Pride_and_Prejudice.txt', encoding='utf8').read()\n","len(book_text)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oZcO2q06yVDr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["book_text[0:500]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jT0ptJlHqZmA","colab_type":"text"},"cell_type":"markdown","source":["### Data Preprocessing"]},{"metadata":{"id":"gGrRS2GskcFG","colab_type":"text"},"cell_type":"markdown","source":["1.Tokenize the data at character level"]},{"metadata":{"id":"WqoU8rqQqZmB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","\n","#Tokenize at character level\n","t = Tokenizer(char_level=True)\n","\n","#Fit tokenizer on the book\n","t.fit_on_texts(book_text)\n","\n","#Vocablury size\n","vocab_size = len(t.word_index)\n","\n","print('Number of unique characters: ', vocab_size)\n","\n","#Convert characters in the book to Numbers\n","book_num = t.texts_to_sequences(book_text)\n","\n","number_chars = len(book_num)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8_cWtApKzWGE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["t.word_index"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fhTY6QPdqZmY","colab_type":"text"},"cell_type":"markdown","source":["## Build Input and Output"]},{"metadata":{"id":"1IGYYK6ZqZmd","colab_type":"text"},"cell_type":"markdown","source":["Input and output container\n","- Input data will have sequences with 100 characters\n","- Output data will have one character which comes after 100 characters in the input data"]},{"metadata":{"id":"XfgONYykqZme","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sequence_length = 100 #Length of input sequence\n","\n","#Empty list for input and output data\n","input_data = []\n","output_data = []\n","\n","#Populate input and output data\n","for i in range(0, number_chars - sequence_length):\n","    #input sequence\n","    input_seq = book_num[i : i + sequence_length]\n","    #Output sequence\n","    output_seq = book_num[i + sequence_length]\n","    \n","    input_data.append(input_seq)\n","    output_data.append(output_seq)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DbMa8MC60lHo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(len(input_data))\n","print(\"Input Data\\n:\",input_data[10])\n","print(\"Output Data\\n:\",output_data[10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QLLac8rUqZmp","colab_type":"text"},"cell_type":"markdown","source":["Reshape and Normalize the input"]},{"metadata":{"id":"QKw7l_fdqZmq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["input_data = np.reshape(input_data, (len(input_data),sequence_length,1))\n","input_data = input_data / vocab_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ME4o6f6YqZm0","colab_type":"text"},"cell_type":"markdown","source":["One hot encode the output"]},{"metadata":{"id":"9GtbzSRgqZm1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.utils import to_categorical\n","output_data = to_categorical(output_data,num_classes=vocab_size+1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rT_trglP4HOe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(output_data[10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f8E7hXgQ6KSV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(input_data.shape[1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fxxyu19V6Rdf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(input_data.shape[2])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wp7_6i4q6aP1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(input_data.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"831Cc5dkqZm7","colab_type":"text"},"cell_type":"markdown","source":["# Build the Model"]},{"metadata":{"id":"5FmHZZrnqZm8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.python.keras.callbacks import LambdaCallback\n","\n","#Build the Model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(input_data.shape[1],input_data.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(vocab_size+1, activation='softmax'))\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy') #No accuracy tracking here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wlmHfhSUqZnW","colab_type":"text"},"cell_type":"markdown","source":["# Execute the model"]},{"metadata":{"id":"lyh80KAdpEXB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Identify a random sequence which we will use to generate output\n","test_seq =  input_data[np.random.randint(0, high=input_data.shape[0])]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fm0pgAzaIa72","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Build a dictionary which can convert numbers into chars\n","int_to_char = dict((i,c) for c, i in t.word_index.items()) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ste0lSBs-igA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def predict_seq(epoch, logs):\n","    \n","    print('Output sequence is: ')\n","    \n","    #Initialize predicted output\n","    predicted_output = ''\n","    \n","    #lets predict 50 next chars\n","    current_seq = np.copy(test_seq)\n","    for i in range(50):\n","        data_input = np.reshape(current_seq,(1,\n","                                             current_seq.shape[0], \n","                                             current_seq.shape[1]))\n","        \n","        #Get the char int with maximum probability\n","        predicted_char_int = np.argmax(model.predict(data_input)[0])\n","        \n","        #Add to the predicted out, convert int to char\n","        predicted_output = predicted_output + int_to_char[predicted_char_int]\n","        \n","        #Update seq with new value at the end\n","        current_seq = np.roll(current_seq, -1)\n","        current_seq[current_seq.shape[0]-1] = [predicted_char_int/vocab_size]\n","    \n","    print(predicted_output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VW72ruTt1I9r","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Create a LabdaCallback to do prediction at end of every epoch\n","checkpoint = LambdaCallback(on_epoch_end=predict_seq)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fObNxWaeLdg5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#Print random starting sequence for prediction\n","print('Initial sequence is: ')\n","for i in range (sequence_length):\n","    print(int_to_char[int(test_seq[i]*vocab_size)], end='')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HAVH-aqyqZna","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model.fit(input_data, output_data, \n","          batch_size=128, \n","          epochs=50,\n","          callbacks=[checkpoint])"],"execution_count":0,"outputs":[]}]}